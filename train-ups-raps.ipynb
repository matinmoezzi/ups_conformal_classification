{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit"
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "colab": {
      "name": "train-ups-raps.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matinmoezzi/ups_conformal_classification/blob/main/train-ups-raps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMh85yYrs3uK",
        "outputId": "1055b269-635c-46de-9a6e-abc01310202c"
      },
      "source": [
        "!git clone https://github.com/matinmoezzi/ups_conformal_classification\n",
        "%cd ups_conformal_classification\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ups_conformal_classification'...\n",
            "remote: Enumerating objects: 42, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 42 (delta 11), reused 29 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (42/42), done.\n",
            "/content/ups_conformal_classification\n",
            "Collecting numpy==1.16.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/e7/6c780e612d245cca62bc3ba8e263038f7c144a96a54f877f3714a0e8427e/numpy-1.16.2-cp37-cp37m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 163kB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.21.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/52/3254e511ef1fc88d31edf457d90ecfd531931d4202f1b8ee0c949e9478f6/scikit_learn-0.21.1-cp37-cp37m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 18.4MB/s \n",
            "\u001b[?25hCollecting scipy==1.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/7e/5cee36eee5b3194687232f6150a89a38f784883c612db7f4da2ab190980d/scipy-1.2.1-cp37-cp37m-manylinux1_x86_64.whl (24.8MB)\n",
            "\u001b[K     |████████████████████████████████| 24.8MB 98kB/s \n",
            "\u001b[?25hCollecting torch==1.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/0b/9d33aef363b6728ad937643d98be713c6c25d50ce338678ad57cee6e6fd5/torch-1.3.0-cp37-cp37m-manylinux1_x86_64.whl (773.1MB)\n",
            "\u001b[K     |██████████████▋                 | 352.4MB 38.4MB/s eta 0:00:11"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UAcwrDrs3uU",
        "outputId": "d5ffbeff-e977-4ca7-eecd-0be99d945e36"
      },
      "source": [
        "%cd ups_conformal_classification"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ups_conformal_classification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igyyCFVfs3uW"
      },
      "source": [
        "import argparse\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "from copy import deepcopy\n",
        "from collections import OrderedDict\n",
        "import pickle\n",
        "import numpy as np\n",
        "from re import search\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, random_split\n",
        "from tensorboardX import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from data.cifar import get_cifar10, get_cifar100\n",
        "from utils import AverageMeter, accuracy\n",
        "from utils.utils import *\n",
        "from utils.train_util import train_initial, train_regular\n",
        "from utils.evaluate import test\n",
        "from utils.pseudo_labeling_util import pseudo_labeling\n",
        "from utils.misc import AverageMeter, accuracy\n",
        "from utils.utils import enable_dropout\n",
        "from conformal_classification.utils import *\n",
        "# from conformal_classification.conformal import *\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoi4l_9Fs3uZ"
      },
      "source": [
        "def raps_pseudo_labeling(args, data_loader, model, itr):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "    end = time.time()\n",
        "    pseudo_idx = []\n",
        "    pseudo_target = []\n",
        "    pseudo_maxstd = []\n",
        "    gt_target = []\n",
        "    idx_list = []\n",
        "    gt_list = []\n",
        "    target_list = []\n",
        "    nl_mask = []\n",
        "    model.eval()\n",
        "    if not args.no_uncertainty:\n",
        "        f_pass = 10\n",
        "        enable_dropout(model)\n",
        "    else:\n",
        "        f_pass = 1\n",
        "\n",
        "    if not args.no_progress:\n",
        "        data_loader = tqdm(data_loader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets, indexs, _) in enumerate(data_loader):\n",
        "            data_time.update(time.time() - end)\n",
        "            inputs = inputs.to(args.device)\n",
        "            targets = targets.to(args.device)\n",
        "            out_prob = []\n",
        "            out_prob_nl = []\n",
        "            for _ in range(f_pass):\n",
        "                outputs, pred_sets = model(inputs)\n",
        "                out_prob.append(F.softmax(outputs, dim=1)) #for selecting positive pseudo-labels\n",
        "                out_prob_nl.append(F.softmax(outputs/args.temp_nl, dim=1)) #for selecting negative pseudo-labels\n",
        "            print(\"pred_sets:\\n\", pred_sets)\n",
        "            out_prob = torch.stack(out_prob)\n",
        "            out_prob_nl = torch.stack(out_prob_nl)\n",
        "            out_std = torch.std(out_prob, dim=0)\n",
        "            out_std_nl = torch.std(out_prob_nl, dim=0)\n",
        "            out_prob = torch.mean(out_prob, dim=0)\n",
        "            out_prob_nl = torch.mean(out_prob_nl, dim=0)\n",
        "            max_value, max_idx = torch.max(out_prob, dim=1)\n",
        "            max_std = out_std.gather(1, max_idx.view(-1,1))\n",
        "            out_std_nl = out_std_nl.cpu().numpy()\n",
        "            \n",
        "            #selecting negative pseudo-labels\n",
        "            interm_nl_mask = ((out_std_nl < args.kappa_n) * (out_prob_nl.cpu().numpy() < args.tau_n)) *1\n",
        "\n",
        "            #manually setting the argmax value to zero\n",
        "            for enum, item in enumerate(max_idx.cpu().numpy()):\n",
        "                interm_nl_mask[enum, item] = 0\n",
        "            nl_mask.extend(interm_nl_mask)\n",
        "\n",
        "            idx_list.extend(indexs.numpy().tolist())\n",
        "            gt_list.extend(targets.cpu().numpy().tolist())\n",
        "            target_list.extend(max_idx.cpu().numpy().tolist())\n",
        "\n",
        "            #selecting positive pseudo-labels\n",
        "            if not args.no_uncertainty:\n",
        "                selected_idx = (max_value>=args.tau_p) * (max_std.squeeze(1) < args.kappa_p)\n",
        "            else:\n",
        "                selected_idx = max_value>=args.tau_p\n",
        "\n",
        "            pseudo_maxstd.extend(max_std.squeeze(1)[selected_idx].cpu().numpy().tolist())\n",
        "            pseudo_target.extend(max_idx[selected_idx].cpu().numpy().tolist())\n",
        "            pseudo_idx.extend(indexs[selected_idx].numpy().tolist())\n",
        "            gt_target.extend(targets[selected_idx].cpu().numpy().tolist())\n",
        "\n",
        "            loss = F.cross_entropy(outputs, targets.to(dtype=torch.long))\n",
        "            prec1, prec5 = accuracy(outputs[selected_idx], targets[selected_idx], topk=(1, 5))\n",
        "\n",
        "            losses.update(loss.item(), inputs.shape[0])\n",
        "            top1.update(prec1.item(), inputs.shape[0])\n",
        "            top5.update(prec5.item(), inputs.shape[0])\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "            if not args.no_progress:\n",
        "                data_loader.set_description(\"Pseudo-Labeling Iter: {batch:4}/{iter:4}. Data: {data:.3f}s. Batch: {bt:.3f}s. Loss: {loss:.4f}. top1: {top1:.2f}. top5: {top5:.2f}. \".format(\n",
        "                    batch=batch_idx + 1,\n",
        "                    iter=len(data_loader),\n",
        "                    data=data_time.avg,\n",
        "                    bt=batch_time.avg,\n",
        "                    loss=losses.avg,\n",
        "                    top1=top1.avg,\n",
        "                    top5=top5.avg,\n",
        "                ))\n",
        "        if not args.no_progress:\n",
        "            data_loader.close()\n",
        "\n",
        "    pseudo_target = np.array(pseudo_target)\n",
        "    gt_target = np.array(gt_target)\n",
        "    pseudo_maxstd = np.array(pseudo_maxstd)\n",
        "    pseudo_idx = np.array(pseudo_idx)\n",
        "\n",
        "    #class balance the selected pseudo-labels\n",
        "    if itr < args.class_blnc-1:\n",
        "        min_count = 5000000 #arbitary large value\n",
        "        for class_idx in range(args.num_classes):\n",
        "            class_len = len(np.where(pseudo_target==class_idx)[0])\n",
        "            if class_len < min_count:\n",
        "                min_count = class_len\n",
        "        min_count = max(25, min_count) #this 25 is used to avoid degenarate cases when the minimum count for a certain class is very low\n",
        "\n",
        "        blnc_idx_list = []\n",
        "        for class_idx in range(args.num_classes):\n",
        "            current_class_idx = np.where(pseudo_target==class_idx)\n",
        "            if len(np.where(pseudo_target==class_idx)[0]) > 0:\n",
        "                current_class_maxstd = pseudo_maxstd[current_class_idx]\n",
        "                sorted_maxstd_idx = np.argsort(current_class_maxstd)\n",
        "                current_class_idx = current_class_idx[0][sorted_maxstd_idx[:min_count]] #select the samples with lowest uncertainty \n",
        "                blnc_idx_list.extend(current_class_idx)\n",
        "\n",
        "        blnc_idx_list = np.array(blnc_idx_list)\n",
        "        pseudo_target = pseudo_target[blnc_idx_list]\n",
        "        pseudo_idx = pseudo_idx[blnc_idx_list]\n",
        "        gt_target = gt_target[blnc_idx_list]\n",
        "\n",
        "    pseudo_labeling_acc = (pseudo_target == gt_target)*1\n",
        "    pseudo_labeling_acc = (sum(pseudo_labeling_acc)/len(pseudo_labeling_acc))*100\n",
        "    print(f'Pseudo-Labeling Accuracy (positive): {pseudo_labeling_acc}, Total Selected: {len(pseudo_idx)}')\n",
        "\n",
        "    pseudo_nl_mask = []\n",
        "    pseudo_nl_idx = []\n",
        "    nl_gt_list = []\n",
        "\n",
        "    for i in range(len(idx_list)):\n",
        "        if idx_list[i] not in pseudo_idx and sum(nl_mask[i]) > 0:\n",
        "            pseudo_nl_mask.append(nl_mask[i])\n",
        "            pseudo_nl_idx.append(idx_list[i])\n",
        "            nl_gt_list.append(gt_list[i])\n",
        "\n",
        "    nl_gt_list = np.array(nl_gt_list)\n",
        "    pseudo_nl_mask = np.array(pseudo_nl_mask)\n",
        "    one_hot_targets = np.eye(args.num_classes)[nl_gt_list]\n",
        "    one_hot_targets = one_hot_targets - 1\n",
        "    one_hot_targets = np.abs(one_hot_targets)\n",
        "    flat_pseudo_nl_mask = pseudo_nl_mask.reshape(1,-1)[0]\n",
        "    flat_one_hot_targets = one_hot_targets.reshape(1,-1)[0]\n",
        "    flat_one_hot_targets = flat_one_hot_targets[np.where(flat_pseudo_nl_mask == 1)]\n",
        "    flat_pseudo_nl_mask = flat_pseudo_nl_mask[np.where(flat_pseudo_nl_mask == 1)]\n",
        "\n",
        "    nl_accuracy = (flat_pseudo_nl_mask == flat_one_hot_targets)*1\n",
        "    nl_accuracy_final = (sum(nl_accuracy)/len(nl_accuracy))*100\n",
        "    print(f'Pseudo-Labeling Accuracy (negative): {nl_accuracy_final}, Total Selected: {len(nl_accuracy)}, Unique Samples: {len(pseudo_nl_mask)}')\n",
        "    pseudo_label_dict = {'pseudo_idx': pseudo_idx.tolist(), 'pseudo_target':pseudo_target.tolist(), 'nl_idx': pseudo_nl_idx, 'nl_mask': pseudo_nl_mask.tolist()}\n",
        " \n",
        "    return losses.avg, top1.avg, pseudo_labeling_acc, len(pseudo_idx), nl_accuracy_final, len(nl_accuracy), len(pseudo_nl_mask), pseudo_label_dict"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKuRn_vIyelb"
      },
      "source": [
        "from PIL import Image\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from data.augmentations import RandAugment, CutoutRandom"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xte33Izvs3up"
      },
      "source": [
        "def get_cifar10_customized(root='data/datasets', n_lbl=4000, ssl_idx=None, pseudo_lbl=None, itr=0, split_txt=''):\n",
        "    os.makedirs(root, exist_ok=True) #create the root directory for saving data\n",
        "    # augmentations\n",
        "    transform_train = transforms.Compose([\n",
        "        RandAugment(3,4),  #from https://arxiv.org/pdf/1909.13719.pdf. For CIFAR-10 M=3, N=4\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(size=32, padding=int(32*0.125), padding_mode='reflect'),\n",
        "        transforms.ColorJitter(\n",
        "            brightness=0.4,\n",
        "            contrast=0.4,\n",
        "            saturation=0.4,\n",
        "        ),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n",
        "        CutoutRandom(n_holes=1, length=16, random=True)\n",
        "    ])\n",
        "    \n",
        "    transform_val = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616))\n",
        "    ])\n",
        "\n",
        "    if ssl_idx is None:\n",
        "        base_dataset = datasets.CIFAR10(root, train=True, download=True)\n",
        "        train_lbl_idx, train_unlbl_idx = lbl_unlbl_split(base_dataset.targets, n_lbl, 10)\n",
        "        \n",
        "        os.makedirs('data/splits', exist_ok=True)\n",
        "        f = open(os.path.join('data/splits', f'cifar10_basesplit_{n_lbl}_{split_txt}.pkl'),\"wb\")\n",
        "        lbl_unlbl_dict = {'lbl_idx': train_lbl_idx, 'unlbl_idx': train_unlbl_idx}\n",
        "        pickle.dump(lbl_unlbl_dict,f)\n",
        "    \n",
        "    else:\n",
        "        lbl_unlbl_dict = pickle.load(open(ssl_idx, 'rb'))\n",
        "        train_lbl_idx = lbl_unlbl_dict['lbl_idx']\n",
        "        train_unlbl_idx = lbl_unlbl_dict['unlbl_idx']\n",
        "\n",
        "    lbl_idx = train_lbl_idx\n",
        "    if pseudo_lbl is not None:\n",
        "        pseudo_lbl_dict = pickle.load(open(pseudo_lbl, 'rb'))\n",
        "        pseudo_idx = pseudo_lbl_dict['pseudo_idx']\n",
        "        pseudo_target = pseudo_lbl_dict['pseudo_target']\n",
        "        nl_idx = pseudo_lbl_dict['nl_idx']\n",
        "        nl_mask = pseudo_lbl_dict['nl_mask']\n",
        "        lbl_idx = np.array(lbl_idx + pseudo_idx)\n",
        "\n",
        "        #balance the labeled and unlabeled data \n",
        "        if len(nl_idx) > len(lbl_idx):\n",
        "            exapand_labeled = len(nl_idx) // len(lbl_idx)\n",
        "            lbl_idx = np.hstack([lbl_idx for _ in range(exapand_labeled)])\n",
        "\n",
        "            if len(lbl_idx) < len(nl_idx):\n",
        "                diff = len(nl_idx) - len(lbl_idx)\n",
        "                lbl_idx = np.hstack((lbl_idx, np.random.choice(lbl_idx, diff)))\n",
        "            else:\n",
        "                assert len(lbl_idx) == len(nl_idx)\n",
        "    else:\n",
        "        pseudo_idx = None\n",
        "        pseudo_target = None\n",
        "        nl_idx = None\n",
        "        nl_mask = None\n",
        "\n",
        "    train_lbl_dataset = CustomizedCIFAR10SSL(\n",
        "        root, lbl_idx, train=True, transform=transform_train,\n",
        "        pseudo_idx=pseudo_idx, pseudo_target=pseudo_target,\n",
        "        nl_idx=nl_idx, nl_mask=nl_mask)\n",
        "    \n",
        "    if nl_idx is not None:\n",
        "        train_nl_dataset = CIFAR10SSL(\n",
        "            root, np.array(nl_idx), train=True, transform=transform_train,\n",
        "            pseudo_idx=pseudo_idx, pseudo_target=pseudo_target,\n",
        "            nl_idx=nl_idx, nl_mask=nl_mask)\n",
        "\n",
        "    return train_lbl_dataset\n",
        "\n",
        "\n",
        "class CustomizedCIFAR10SSL(datasets.CIFAR10):\n",
        "    def __init__(self, root, indexs, train=True,\n",
        "                 transform=None, target_transform=None,\n",
        "                 download=True, pseudo_idx=None, pseudo_target=None,\n",
        "                 nl_idx=None, nl_mask=None):\n",
        "        super().__init__(root, train=train,\n",
        "                         transform=transform,\n",
        "                         target_transform=target_transform,\n",
        "                         download=download)\n",
        "        \n",
        "        self.targets = np.array(self.targets)\n",
        "        self.nl_mask = np.ones((len(self.targets), len(np.unique(self.targets))))\n",
        "        \n",
        "        if nl_mask is not None:\n",
        "            self.nl_mask[nl_idx] = nl_mask\n",
        "\n",
        "        if pseudo_target is not None:\n",
        "            self.targets[pseudo_idx] = pseudo_target\n",
        "\n",
        "        if indexs is not None:\n",
        "            indexs = np.array(indexs)\n",
        "            self.data = self.data[indexs]\n",
        "            self.targets = np.array(self.targets)[indexs]\n",
        "            self.nl_mask = np.array(self.nl_mask)[indexs]\n",
        "            self.indexs = indexs\n",
        "        else:\n",
        "            self.indexs = np.arange(len(self.targets))\n",
        "        \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4LwpFZQ5nJ2"
      },
      "source": [
        "from conformal_classification.utils import validate, get_logits_targets, sort_sum\n",
        "from scipy.special import softmax"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e31X7YtV5f6U"
      },
      "source": [
        "class ConformalModel(nn.Module):\n",
        "    def __init__(self, model, calib_loader, alpha, kreg=None, lamda=None, randomized=True, allow_zero_sets=False, pct_paramtune=0.3, batch_size=32, lamda_criterion='size'):\n",
        "        super(ConformalModel, self).__init__()\n",
        "        self.model = model\n",
        "        self.alpha = alpha\n",
        "        # initialize (1.3 is usually a good value)\n",
        "        self.T = torch.Tensor([1.3])\n",
        "        self.T, calib_logits = platt(self, calib_loader)\n",
        "        self.randomized = randomized\n",
        "        self.allow_zero_sets = allow_zero_sets\n",
        "        self.num_classes = len(calib_loader.dataset.classes)\n",
        "\n",
        "        if kreg == None or lamda == None:\n",
        "            kreg, lamda, calib_logits = pick_parameters(\n",
        "                model, calib_logits, alpha, kreg, lamda, randomized, allow_zero_sets, pct_paramtune, batch_size, lamda_criterion)\n",
        "\n",
        "        self.penalties = np.zeros((1, self.num_classes))\n",
        "        self.penalties[:, kreg:] += lamda\n",
        "\n",
        "        calib_loader = DataLoader(\n",
        "            calib_logits, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "        self.Qhat = conformal_calibration_logits(self, calib_loader)\n",
        "\n",
        "    def forward(self, *args, randomized=None, allow_zero_sets=None, **kwargs):\n",
        "        if randomized == None:\n",
        "            randomized = self.randomized\n",
        "        if allow_zero_sets == None:\n",
        "            allow_zero_sets = self.allow_zero_sets\n",
        "        logits = self.model(*args, **kwargs)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits_numpy = logits.detach().cpu().numpy()\n",
        "            scores = softmax(logits_numpy/self.T.item(), axis=1)\n",
        "\n",
        "            I, ordered, cumsum = sort_sum(scores)\n",
        "\n",
        "            S = gcq(scores, self.Qhat, I=I, ordered=ordered, cumsum=cumsum,\n",
        "                    penalties=self.penalties, randomized=randomized, allow_zero_sets=allow_zero_sets)\n",
        "\n",
        "        return logits, S\n",
        "\n",
        "# Computes the conformal calibration\n",
        "\n",
        "\n",
        "def conformal_calibration(cmodel, calib_loader):\n",
        "    print(\"Conformal calibration\")\n",
        "    with torch.no_grad():\n",
        "        E = np.array([])\n",
        "        for x, targets in tqdm(calib_loader):\n",
        "            logits = cmodel.model(x.cuda()).detach().cpu().numpy()\n",
        "            scores = softmax(logits/cmodel.T.item(), axis=1)\n",
        "\n",
        "            I, ordered, cumsum = sort_sum(scores)\n",
        "\n",
        "            E = np.concatenate((E, giq(scores, targets, I=I, ordered=ordered, cumsum=cumsum,\n",
        "                                       penalties=cmodel.penalties, randomized=True, allow_zero_sets=True)))\n",
        "\n",
        "        Qhat = np.quantile(E, 1-cmodel.alpha, interpolation='higher')\n",
        "\n",
        "        return Qhat\n",
        "\n",
        "# Temperature scaling\n",
        "\n",
        "\n",
        "def platt(cmodel, calib_loader, max_iters=10, lr=0.01, epsilon=0.01):\n",
        "    print(\"Begin Platt scaling.\")\n",
        "    # Save logits so don't need to double compute them\n",
        "    logits_dataset = get_logits_targets(cmodel.model, calib_loader)\n",
        "    logits_loader = torch.utils.data.DataLoader(\n",
        "        logits_dataset, batch_size=calib_loader.batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "    T = platt_logits(cmodel, logits_loader,\n",
        "                     max_iters=max_iters, lr=lr, epsilon=epsilon)\n",
        "\n",
        "    print(f\"Optimal T={T.item()}\")\n",
        "    return T, logits_dataset\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "        INTERNAL FUNCTIONS\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Precomputed-logit versions of the above functions.\n",
        "\n",
        "\n",
        "class ConformalModelLogits(nn.Module):\n",
        "    def __init__(self, model, calib_loader, alpha, kreg=None, lamda=None, randomized=True, allow_zero_sets=False, naive=False, LAC=False, pct_paramtune=0.3, batch_size=32, lamda_criterion='size'):\n",
        "        super(ConformalModelLogits, self).__init__()\n",
        "        self.model = model\n",
        "        self.alpha = alpha\n",
        "        self.randomized = randomized\n",
        "        self.LAC = LAC\n",
        "        self.allow_zero_sets = allow_zero_sets\n",
        "        self.T = platt_logits(self, calib_loader)\n",
        "\n",
        "        if (kreg == None or lamda == None) and not naive and not LAC:\n",
        "            kreg, lamda, calib_logits = pick_parameters(\n",
        "                model, calib_loader.dataset, alpha, kreg, lamda, randomized, allow_zero_sets, pct_paramtune, batch_size, lamda_criterion)\n",
        "            calib_loader = DataLoader(\n",
        "                calib_logits, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "        self.penalties = np.zeros((1, calib_loader.dataset[0][0].shape[0]))\n",
        "        if not (kreg == None) and not naive and not LAC:\n",
        "            self.penalties[:, kreg:] += lamda\n",
        "        self.Qhat = 1-alpha\n",
        "        if not naive and not LAC:\n",
        "            self.Qhat = conformal_calibration_logits(self, calib_loader)\n",
        "        elif not naive and LAC:\n",
        "            gt_locs_cal = np.array([np.where(np.argsort(x[0]).flip(dims=(0,)) == x[1])[\n",
        "                                   0][0] for x in calib_loader.dataset])\n",
        "            scores_cal = 1-np.array([np.sort(torch.softmax(calib_loader.dataset[i][0]/self.T.item(), dim=0))[\n",
        "                                    ::-1][gt_locs_cal[i]] for i in range(len(calib_loader.dataset))])\n",
        "            self.Qhat = np.quantile(scores_cal, np.ceil(\n",
        "                (scores_cal.shape[0]+1) * (1-alpha)) / scores_cal.shape[0])\n",
        "\n",
        "    def forward(self, logits, randomized=None, allow_zero_sets=None):\n",
        "        if randomized == None:\n",
        "            randomized = self.randomized\n",
        "        if allow_zero_sets == None:\n",
        "            allow_zero_sets = self.allow_zero_sets\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits_numpy = logits.detach().cpu().numpy()\n",
        "            scores = softmax(logits_numpy/self.T.item(), axis=1)\n",
        "\n",
        "            if not self.LAC:\n",
        "                I, ordered, cumsum = sort_sum(scores)\n",
        "\n",
        "                S = gcq(scores, self.Qhat, I=I, ordered=ordered, cumsum=cumsum,\n",
        "                        penalties=self.penalties, randomized=randomized, allow_zero_sets=allow_zero_sets)\n",
        "            else:\n",
        "                S = [np.where((1-scores[i, :]) < self.Qhat)[0]\n",
        "                     for i in range(scores.shape[0])]\n",
        "\n",
        "        return logits, S\n",
        "\n",
        "\n",
        "def conformal_calibration_logits(cmodel, calib_loader):\n",
        "    with torch.no_grad():\n",
        "        E = np.array([])\n",
        "        for logits, targets in calib_loader:\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "\n",
        "            scores = softmax(logits/cmodel.T.item(), axis=1)\n",
        "\n",
        "            I, ordered, cumsum = sort_sum(scores)\n",
        "\n",
        "            E = np.concatenate((E, giq(scores, targets, I=I, ordered=ordered, cumsum=cumsum,\n",
        "                                       penalties=cmodel.penalties, randomized=True, allow_zero_sets=True)))\n",
        "\n",
        "        Qhat = np.quantile(E, 1-cmodel.alpha, interpolation='higher')\n",
        "\n",
        "        return Qhat\n",
        "\n",
        "\n",
        "def platt_logits(cmodel, calib_loader, max_iters=10, lr=0.01, epsilon=0.01):\n",
        "    nll_criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    T = nn.Parameter(torch.Tensor([1.3]).cuda())\n",
        "\n",
        "    optimizer = optim.SGD([T], lr=lr)\n",
        "    for iter in range(max_iters):\n",
        "        T_old = T.item()\n",
        "        for x, targets in calib_loader:\n",
        "            optimizer.zero_grad()\n",
        "            x = x.cuda()\n",
        "            x.requires_grad = True\n",
        "            out = x/T\n",
        "            loss = nll_criterion(out, targets.long().cuda())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        if abs(T_old - T.item()) < epsilon:\n",
        "            break\n",
        "    return T\n",
        "\n",
        "# CORE CONFORMAL INFERENCE FUNCTIONS\n",
        "\n",
        "# Generalized conditional quantile function.\n",
        "\n",
        "\n",
        "def gcq(scores, tau, I, ordered, cumsum, penalties, randomized, allow_zero_sets):\n",
        "    penalties_cumsum = np.cumsum(penalties, axis=1)\n",
        "    sizes_base = ((cumsum + penalties_cumsum) <=\n",
        "                  tau).sum(axis=1) + 1  # 1 - 1001\n",
        "    sizes_base = np.minimum(sizes_base, scores.shape[1])  # 1-1000\n",
        "\n",
        "    if randomized:\n",
        "        V = np.zeros(sizes_base.shape)\n",
        "        for i in range(sizes_base.shape[0]):\n",
        "            V[i] = 1/ordered[i, sizes_base[i]-1] * \\\n",
        "                (tau-(cumsum[i, sizes_base[i]-1]-ordered[i, sizes_base[i]-1]) -\n",
        "                 penalties_cumsum[0, sizes_base[i]-1])  # -1 since sizes_base \\in {1,...,1000}.\n",
        "\n",
        "        sizes = sizes_base - (np.random.random(V.shape) >= V).astype(int)\n",
        "    else:\n",
        "        sizes = sizes_base\n",
        "\n",
        "    if tau == 1.0:\n",
        "        # always predict max size if alpha==0. (Avoids numerical error.)\n",
        "        sizes[:] = cumsum.shape[1]\n",
        "\n",
        "    if not allow_zero_sets:\n",
        "        # allow the user the option to never have empty sets (will lead to incorrect coverage if 1-alpha < model's top-1 accuracy\n",
        "        sizes[sizes == 0] = 1\n",
        "\n",
        "    S = list()\n",
        "\n",
        "    # Construct S from equation (5)\n",
        "    for i in range(I.shape[0]):\n",
        "        S = S + [I[i, 0:sizes[i]], ]\n",
        "\n",
        "    return S\n",
        "\n",
        "# Get the 'p-value'\n",
        "\n",
        "\n",
        "def get_tau(score, target, I, ordered, cumsum, penalty, randomized, allow_zero_sets):  # For one example\n",
        "    idx = np.where(I == target)\n",
        "    tau_nonrandom = cumsum[idx]\n",
        "\n",
        "    if not randomized:\n",
        "        return tau_nonrandom + penalty[0]\n",
        "\n",
        "    U = np.random.random()\n",
        "\n",
        "    if idx == (0, 0):\n",
        "        if not allow_zero_sets:\n",
        "            return tau_nonrandom + penalty[0]\n",
        "        else:\n",
        "            return U * tau_nonrandom + penalty[0]\n",
        "    else:\n",
        "        return U * ordered[idx] + cumsum[(idx[0], idx[1]-1)] + (penalty[0:(idx[1][0]+1)]).sum()\n",
        "\n",
        "# Gets the histogram of Taus.\n",
        "\n",
        "\n",
        "def giq(scores, targets, I, ordered, cumsum, penalties, randomized, allow_zero_sets):\n",
        "    \"\"\"\n",
        "        Generalized inverse quantile conformity score function.\n",
        "        E from equation (7) in Romano, Sesia, Candes.  Find the minimum tau in [0, 1] such that the correct label enters.\n",
        "    \"\"\"\n",
        "    E = -np.ones((scores.shape[0],))\n",
        "    for i in range(scores.shape[0]):\n",
        "        E[i] = get_tau(scores[i:i+1, :], targets[i].item(), I[i:i+1, :], ordered[i:i+1, :],\n",
        "                       cumsum[i:i+1, :], penalties[0, :], randomized=randomized, allow_zero_sets=allow_zero_sets)\n",
        "\n",
        "    return E\n",
        "\n",
        "# AUTOMATIC PARAMETER TUNING FUNCTIONS\n",
        "\n",
        "\n",
        "def pick_kreg(paramtune_logits, alpha):\n",
        "    gt_locs_kstar = np.array([np.where(np.argsort(x[0]).flip(dims=(0,)) == x[1])[\n",
        "                             0][0] for x in paramtune_logits])\n",
        "    kstar = np.quantile(gt_locs_kstar, 1-alpha, interpolation='higher') + 1\n",
        "    return kstar\n",
        "\n",
        "\n",
        "def pick_lamda_size(model, paramtune_loader, alpha, kreg, randomized, allow_zero_sets):\n",
        "    # Calculate lamda_star\n",
        "    best_size = iter(paramtune_loader).__next__()[\n",
        "        0][1].shape[0]  # number of classes\n",
        "    # Use the paramtune data to pick lamda.  Does not violate exchangeability.\n",
        "    # predefined grid, change if more precision desired.\n",
        "    for temp_lam in [0.001, 0.01, 0.1, 0.2, 0.5]:\n",
        "        conformal_model = ConformalModelLogits(model, paramtune_loader, alpha=alpha, kreg=kreg,\n",
        "                                               lamda=temp_lam, randomized=randomized, allow_zero_sets=allow_zero_sets, naive=False)\n",
        "        top1_avg, top5_avg, cvg_avg, sz_avg = validate(\n",
        "            paramtune_loader, conformal_model, print_bool=False)\n",
        "        if sz_avg < best_size:\n",
        "            best_size = sz_avg\n",
        "            lamda_star = temp_lam\n",
        "    return lamda_star\n",
        "\n",
        "\n",
        "def pick_lamda_adaptiveness(model, paramtune_loader, alpha, kreg, randomized, allow_zero_sets, strata=[[0, 1], [2, 3], [4, 6], [7, 10], [11, 100], [101, 1000]]):\n",
        "    # Calculate lamda_star\n",
        "    lamda_star = 0\n",
        "    best_violation = 1\n",
        "    # Use the paramtune data to pick lamda.  Does not violate exchangeability.\n",
        "    # predefined grid, change if more precision desired.\n",
        "    for temp_lam in [0, 1e-5, 1e-4, 8e-4, 9e-4, 1e-3, 1.5e-3, 2e-3]:\n",
        "        conformal_model = ConformalModelLogits(model, paramtune_loader, alpha=alpha, kreg=kreg,\n",
        "                                               lamda=temp_lam, randomized=randomized, allow_zero_sets=allow_zero_sets, naive=False)\n",
        "        curr_violation = get_violation(\n",
        "            conformal_model, paramtune_loader, strata, alpha)\n",
        "        if curr_violation < best_violation:\n",
        "            best_violation = curr_violation\n",
        "            lamda_star = temp_lam\n",
        "    return lamda_star\n",
        "\n",
        "\n",
        "def pick_parameters(model, calib_logits, alpha, kreg, lamda, randomized, allow_zero_sets, pct_paramtune, batch_size, lamda_criterion):\n",
        "    num_paramtune = int(np.ceil(pct_paramtune * len(calib_logits)))\n",
        "    paramtune_logits, calib_logits = random_split(\n",
        "        calib_logits, [num_paramtune, len(calib_logits)-num_paramtune])\n",
        "    calib_loader = DataLoader(\n",
        "        calib_logits, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "    paramtune_loader = DataLoader(\n",
        "        calib_logits, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "    if kreg == None:\n",
        "        kreg = pick_kreg(paramtune_logits, alpha)\n",
        "    if lamda == None:\n",
        "        if lamda_criterion == \"size\":\n",
        "            lamda = pick_lamda_size(\n",
        "                model, paramtune_loader, alpha, kreg, randomized, allow_zero_sets)\n",
        "        elif lamda_criterion == \"adaptiveness\":\n",
        "            lamda = pick_lamda_adaptiveness(\n",
        "                model, paramtune_loader, alpha, kreg, randomized, allow_zero_sets)\n",
        "    return kreg, lamda, calib_logits\n",
        "\n",
        "\n",
        "def get_violation(cmodel, loader_paramtune, strata, alpha):\n",
        "    df = pd.DataFrame(columns=['size', 'correct'])\n",
        "    for logit, target in loader_paramtune:\n",
        "        # compute output\n",
        "        # This is a 'dummy model' which takes logits, for efficiency.\n",
        "        output, S = cmodel(logit)\n",
        "        # measure accuracy and record loss\n",
        "        size = np.array([x.size for x in S])\n",
        "        I, _, _ = sort_sum(logit.numpy())\n",
        "        correct = np.zeros_like(size)\n",
        "        for j in range(correct.shape[0]):\n",
        "            correct[j] = int(target[j] in list(S[j]))\n",
        "        batch_df = pd.DataFrame({'size': size, 'correct': correct})\n",
        "        df = df.append(batch_df, ignore_index=True)\n",
        "    wc_violation = 0\n",
        "    for stratum in strata:\n",
        "        temp_df = df[(df['size'] >= stratum[0]) & (df['size'] <= stratum[1])]\n",
        "        if len(temp_df) == 0:\n",
        "            continue\n",
        "        stratum_violation = abs(temp_df.correct.mean()-(1-alpha))\n",
        "        wc_violation = max(wc_violation, stratum_violation)\n",
        "    return wc_violation  # the violation"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDIDI9z3s3u0"
      },
      "source": [
        "cudnn.benchmark = True"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "XGLYA_G4s3u8",
        "outputId": "5dd1bfc9-1ad0-45e0-c835-418203e77a0a"
      },
      "source": [
        "run_started = datetime.today().strftime('%d-%m-%y_%H%M') #start time to create unique experiment name\n",
        "parser = argparse.ArgumentParser(description='UPS Training')\n",
        "parser.add_argument('--out', default=f'outputs', help='directory to output the result')\n",
        "parser.add_argument('--gpu-id', default='0', type=int,\n",
        "                    help='id(s) for CUDA_VISIBLE_DEVICES')\n",
        "parser.add_argument('--num-workers', type=int, default=8,\n",
        "                    help='number of workers')\n",
        "parser.add_argument('--dataset', default='cifar10', type=str,\n",
        "                    choices=['cifar10', 'cifar100'],\n",
        "                    help='dataset names')\n",
        "parser.add_argument('--n-lbl', type=int, default=4000,\n",
        "                    help='number of labeled data')\n",
        "parser.add_argument('--arch', default='cnn13', type=str,\n",
        "                    choices=['wideresnet', 'cnn13', 'shakeshake'],\n",
        "                    help='architecture name')\n",
        "parser.add_argument('--iterations', default=20, type=int,\n",
        "                    help='number of total pseudo-labeling iterations to run')\n",
        "parser.add_argument('--epchs', default=1024, type=int,\n",
        "                    help='number of total epochs to run')\n",
        "parser.add_argument('--start-epoch', default=0, type=int,\n",
        "                    help='manual epoch number (useful on restarts)')\n",
        "parser.add_argument('--batchsize', default=128, type=int,\n",
        "                    help='train batchsize')\n",
        "parser.add_argument('--lr', '--learning-rate', default=0.03, type=float,\n",
        "                    help='initial learning rate, default 0.03')\n",
        "parser.add_argument('--warmup', default=0, type=float,\n",
        "                    help='warmup epochs (unlabeled data based)')\n",
        "parser.add_argument('--wdecay', default=5e-4, type=float,\n",
        "                    help='weight decay')\n",
        "parser.add_argument('--nesterov', action='store_true', default=True,\n",
        "                    help='use nesterov momentum')\n",
        "parser.add_argument('--resume', default='', type=str,\n",
        "                    help='path to latest checkpoint (default: none)')\n",
        "parser.add_argument('--seed', type=int, default=-1,\n",
        "                    help=\"random seed (-1: don't use random seed)\")\n",
        "parser.add_argument('--no-progress', action='store_true',\n",
        "                    help=\"don't use progress bar\")\n",
        "parser.add_argument('--dropout', default=0.3, type=float,\n",
        "                    help='dropout probs')\n",
        "parser.add_argument('--num-classes', default=10, type=int,\n",
        "                    help='total classes')\n",
        "parser.add_argument('--class-blnc', default=10, type=int,\n",
        "                    help='total number of class balanced iterations')\n",
        "parser.add_argument('--tau-p', default=0.70, type=float,\n",
        "                    help='confidece threshold for positive pseudo-labels, default 0.70')\n",
        "parser.add_argument('--tau-n', default=0.05, type=float,\n",
        "                    help='confidece threshold for negative pseudo-labels, default 0.05')\n",
        "parser.add_argument('--kappa-p', default=0.05, type=float,\n",
        "                    help='uncertainty threshold for positive pseudo-labels, default 0.05')\n",
        "parser.add_argument('--kappa-n', default=0.005, type=float,\n",
        "                    help='uncertainty threshold for negative pseudo-labels, default 0.005')\n",
        "parser.add_argument('--temp-nl', default=2.0, type=float,\n",
        "                    help='temperature for generating negative pseduo-labels, default 2.0')\n",
        "parser.add_argument('--no-uncertainty', action='store_true',\n",
        "                    help='use uncertainty in the pesudo-label selection, default true')\n",
        "parser.add_argument('--split-txt', default='run1', type=str,\n",
        "                    help='extra text to differentiate different experiments. it also creates a new labeled/unlabeled split')\n",
        "parser.add_argument('--model-width', default=2, type=int,\n",
        "                    help='model width for WRN-28')\n",
        "parser.add_argument('--model-depth', default=28, type=int,\n",
        "                    help='model depth for WRN')\n",
        "parser.add_argument('--test-freq', default=10, type=int,\n",
        "                    help='frequency of evaluations')\n",
        "\n",
        "\n",
        "options = (\"--dataset\", \"cifar10\", \"--n-lbl\", \"1000\", \"--class-blnc\", \"7\", \"--split-txt\", \"run1\", \"--arch\", \"cnn13\")\n",
        "args = parser.parse_args(options)\n",
        "#print key configurations\n",
        "print('########################################################################')\n",
        "print('########################################################################')\n",
        "print(f'dataset:                                  {args.dataset}')\n",
        "print(f'number of labeled samples:                {args.n_lbl}')\n",
        "print(f'architecture:                             {args.arch}')\n",
        "print(f'number of pseudo-labeling iterations:     {args.iterations}')\n",
        "print(f'number of epochs:                         {args.epchs}')\n",
        "print(f'batch size:                               {args.batchsize}')\n",
        "print(f'lr:                                       {args.lr}')\n",
        "print(f'value of tau_p:                           {args.tau_p}')\n",
        "print(f'value of tau_n:                           {args.tau_n}')\n",
        "print(f'value of kappa_p:                         {args.kappa_p}')\n",
        "print(f'value of kappa_n:                         {args.kappa_n}')\n",
        "print('########################################################################')\n",
        "print('########################################################################')\n",
        "\n",
        "DATASET_GETTERS = {'cifar10': get_cifar10, 'cifar100': get_cifar100}\n",
        "exp_name = f'exp_{args.dataset}_{args.n_lbl}_{args.arch}_{args.split_txt}_{args.epchs}_{args.class_blnc}_{args.tau_p}_{args.tau_n}_{args.kappa_p}_{args.kappa_n}_{run_started}'\n",
        "# device = torch.device('cuda', args.gpu_id)\n",
        "device = torch.device('cpu')\n",
        "args.device = device\n",
        "args.exp_name = exp_name\n",
        "args.dtype = torch.float32\n",
        "if args.seed != -1:\n",
        "    set_seed(args)\n",
        "args.out = os.path.join(args.out, args.exp_name)\n",
        "start_itr = 0\n",
        "\n",
        "if args.resume and os.path.isdir(args.resume):\n",
        "    resume_files = os.listdir(args.resume)\n",
        "    resume_itrs = [int(item.replace('.pkl','').split(\"_\")[-1]) for item in resume_files if 'pseudo_labeling_iteration' in item]\n",
        "    if len(resume_itrs) > 0:\n",
        "        start_itr = max(resume_itrs)\n",
        "    args.out = args.resume\n",
        "os.makedirs(args.out, exist_ok=True)\n",
        "writer = SummaryWriter(args.out)\n",
        "\n",
        "if args.dataset == 'cifar10':\n",
        "    args.num_classes = 10\n",
        "elif args.dataset == 'cifar100':\n",
        "    args.num_classes = 100\n",
        "\n",
        "for itr in range(start_itr, args.iterations):\n",
        "    if itr == 0 and args.n_lbl < 4000: #use a smaller batchsize to increase the number of iterations\n",
        "        args.batch_size = 64\n",
        "        args.epochs = 1024\n",
        "    else:\n",
        "        args.batch_size = args.batchsize\n",
        "        args.epochs = args.epchs\n",
        "\n",
        "    if os.path.exists(f'data/splits/{args.dataset}_basesplit_{args.n_lbl}_{args.split_txt}.pkl'):\n",
        "        lbl_unlbl_split = f'data/splits/{args.dataset}_basesplit_{args.n_lbl}_{args.split_txt}.pkl'\n",
        "    else:\n",
        "        lbl_unlbl_split = None\n",
        "    \n",
        "    #load the saved pseudo-labels\n",
        "    if itr > 0:\n",
        "        pseudo_lbl_dict = f'{args.out}/pseudo_labeling_iteration_{str(itr)}.pkl'\n",
        "    else:\n",
        "        pseudo_lbl_dict = None\n",
        "    \n",
        "    lbl_dataset, nl_dataset, unlbl_dataset, test_dataset = DATASET_GETTERS[args.dataset]('data/datasets', args.n_lbl,\n",
        "                                                            lbl_unlbl_split, pseudo_lbl_dict, itr, args.split_txt)\n",
        "\n",
        "    model = create_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    nl_batchsize = int((float(args.batch_size) * len(nl_dataset))/(len(lbl_dataset) + len(nl_dataset)))\n",
        "\n",
        "    if itr == 0:\n",
        "        lbl_batchsize = args.batch_size\n",
        "        args.iteration = len(lbl_dataset) // args.batch_size\n",
        "    else:\n",
        "        lbl_batchsize = args.batch_size - nl_batchsize\n",
        "        args.iteration = (len(lbl_dataset) + len(nl_dataset)) // args.batch_size\n",
        "\n",
        "    lbl_loader = DataLoader(\n",
        "        lbl_dataset,\n",
        "        sampler=RandomSampler(lbl_dataset),\n",
        "        batch_size=lbl_batchsize,\n",
        "        num_workers=args.num_workers,\n",
        "        drop_last=True)\n",
        "\n",
        "    nl_loader = DataLoader(\n",
        "        nl_dataset,\n",
        "        sampler=RandomSampler(nl_dataset),\n",
        "        batch_size=nl_batchsize,\n",
        "        num_workers=args.num_workers,\n",
        "        drop_last=True)\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        sampler=SequentialSampler(test_dataset),\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=args.num_workers)\n",
        "    \n",
        "    unlbl_loader = DataLoader(\n",
        "        unlbl_dataset,\n",
        "        sampler=SequentialSampler(unlbl_dataset),\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=args.num_workers)\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, nesterov=args.nesterov)\n",
        "    args.total_steps = args.epochs * args.iteration\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, args.warmup * args.iteration, args.total_steps)\n",
        "    start_epoch = 0\n",
        "\n",
        "    if args.resume and itr == start_itr and os.path.isdir(args.resume):\n",
        "        resume_itrs = [int(item.replace('.pth.tar','').split(\"_\")[-1]) for item in resume_files if 'checkpoint_iteration_' in item]\n",
        "        if len(resume_itrs) > 0:\n",
        "            checkpoint_itr = max(resume_itrs)\n",
        "            resume_model = os.path.join(args.resume, f'checkpoint_iteration_{checkpoint_itr}.pth.tar')\n",
        "            if os.path.isfile(resume_model) and checkpoint_itr == itr:\n",
        "                checkpoint = torch.load(resume_model)\n",
        "                best_acc = checkpoint['best_acc']\n",
        "                start_epoch = checkpoint['epoch']\n",
        "                model.load_state_dict(checkpoint['state_dict'])\n",
        "                optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "                scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "\n",
        "    model.zero_grad()\n",
        "    best_acc = 0\n",
        "\n",
        "    args.epochs = 1\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "        if itr == 0:\n",
        "            train_loss = train_initial(args, lbl_loader, model, optimizer, scheduler, epoch, itr)\n",
        "        else:\n",
        "            train_loss = train_regular(args, lbl_loader, nl_loader, model, optimizer, scheduler, epoch, itr)\n",
        "\n",
        "        test_loss = 0.0\n",
        "        test_acc = 0.0\n",
        "        test_model = model\n",
        "        if epoch > (args.epochs+1)/2 and epoch%args.test_freq==0:\n",
        "            test_loss, test_acc = test(args, test_loader, test_model)\n",
        "        elif epoch == (args.epochs-1):\n",
        "            test_loss, test_acc = test(args, test_loader, test_model)\n",
        "\n",
        "        writer.add_scalar('train/1.train_loss', train_loss, (itr*args.epochs)+epoch)\n",
        "        writer.add_scalar('test/1.test_acc', test_acc, (itr*args.epochs)+epoch)\n",
        "        writer.add_scalar('test/2.test_loss', test_loss, (itr*args.epochs)+epoch)\n",
        "\n",
        "        is_best = test_acc > best_acc\n",
        "        best_acc = max(test_acc, best_acc)\n",
        "        model_to_save = model.module if hasattr(model, \"module\") else model\n",
        "        save_checkpoint({\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model_to_save.state_dict(),\n",
        "            'acc': test_acc,\n",
        "            'best_acc': best_acc,\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'scheduler': scheduler.state_dict(),\n",
        "        }, is_best, args.out, f'iteration_{str(itr)}')\n",
        "\n",
        "    checkpoint = torch.load(f'{args.out}/checkpoint_iteration_{str(itr)}.pth.tar')\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    model.zero_grad()\n",
        "\n",
        "    ## create calibration holdout data\n",
        "    lbl_dataset_custom = get_cifar10_customized('data/datasets', args.n_lbl, lbl_unlbl_split, pseudo_lbl_dict, itr, args.split_txt)\n",
        "    calib_lbl_loader = DataLoader(\n",
        "        lbl_dataset_custom,\n",
        "        sampler=RandomSampler(lbl_dataset_custom),\n",
        "        batch_size=lbl_batchsize,\n",
        "        num_workers=args.num_workers,\n",
        "        drop_last=True)\n",
        "    conformal_model = ConformalModel(model, calib_lbl_loader, alpha=0.1, lamda_criterion='size') # conformal model for generating pseudo-labels\n",
        "\n",
        "    #pseudo-label generation and selection\n",
        "    pl_loss, pl_acc, pl_acc_pos, total_sel_pos, pl_acc_neg, total_sel_neg, unique_sel_neg, pseudo_label_dict = pseudo_labeling(args, unlbl_loader, model, itr)\n",
        "\n",
        "    writer.add_scalar('pseudo_labeling/1.regular_loss', pl_loss, itr)\n",
        "    writer.add_scalar('pseudo_labeling/2.regular_acc', pl_acc, itr)\n",
        "    writer.add_scalar('pseudo_labeling/3.pseudo_acc_positive', pl_acc_pos, itr)\n",
        "    writer.add_scalar('pseudo_labeling/4.total_sel_positive', total_sel_pos, itr)\n",
        "    writer.add_scalar('pseudo_labeling/5.pseudo_acc_negative', pl_acc_neg, itr)\n",
        "    writer.add_scalar('pseudo_labeling/6.total_sel_negative', total_sel_neg, itr)\n",
        "    writer.add_scalar('pseudo_labeling/7.unique_samples_negative', unique_sel_neg, itr)\n",
        "\n",
        "    with open(os.path.join(args.out, f'pseudo_labeling_iteration_{str(itr+1)}.pkl'),\"wb\") as f:\n",
        "        pickle.dump(pseudo_label_dict,f)\n",
        "    \n",
        "    with open(os.path.join(args.out, 'log.txt'), 'a+') as ofile:\n",
        "        ofile.write(f'############################# PL Iteration: {itr+1} #############################\\n')\n",
        "        ofile.write(f'Last Test Acc: {test_acc}, Best Test Acc: {best_acc}\\n')\n",
        "        ofile.write(f'PL Acc (Positive): {pl_acc_pos}, Total Selected (Positive): {total_sel_pos}\\n')\n",
        "        ofile.write(f'PL Acc (Negative): {pl_acc_neg}, Total Selected (Negative): {total_sel_neg}, Unique Negative Samples: {unique_sel_neg}\\n\\n')\n",
        "\n",
        "writer.close()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pseudo-Labeling Iter:   81/ 766. Data: 0.012s. Batch: 0.168s. Loss: 2.2511. top1: 0.00. top5: 0.00. :  11%|█         | 81/766 [00:13<01:51,  6.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-0364988a4b1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;31m#pseudo-label generation and selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0mpl_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl_acc_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sel_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl_acc_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sel_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_sel_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpseudo_label_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpseudo_labeling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlbl_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pseudo_labeling/1.regular_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ups_conformal_classification/utils/pseudo_labeling_util.py\u001b[0m in \u001b[0;36mpseudo_labeling\u001b[0;34m(args, data_loader, model, itr)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mmax_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mmax_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_std\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mout_std_nl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_std_nl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m#selecting negative pseudo-labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpsPQuUDvlTA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}