{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "import numpy as np\n",
    "from re import search\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from data.cifar import get_cifar10, get_cifar100\n",
    "from utils import AverageMeter, accuracy\n",
    "from utils.utils import *\n",
    "from utils.train_util import train_initial, train_regular\n",
    "from utils.evaluate import test\n",
    "from utils.pseudo_labeling_util import pseudo_labeling\n",
    "from .misc import AverageMeter, accuracy\n",
    "from .utils import enable_dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_labeling(args, data_loader, model, itr):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "    pseudo_idx = []\n",
    "    pseudo_target = []\n",
    "    pseudo_maxstd = []\n",
    "    gt_target = []\n",
    "    idx_list = []\n",
    "    gt_list = []\n",
    "    target_list = []\n",
    "    nl_mask = []\n",
    "    model.eval()\n",
    "    if not args.no_uncertainty:\n",
    "        f_pass = 10\n",
    "        enable_dropout(model)\n",
    "    else:\n",
    "        f_pass = 1\n",
    "\n",
    "    if not args.no_progress:\n",
    "        data_loader = tqdm(data_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets, indexs, _) in enumerate(data_loader):\n",
    "            data_time.update(time.time() - end)\n",
    "            inputs = inputs.to(args.device)\n",
    "            targets = targets.to(args.device)\n",
    "            out_prob = []\n",
    "            out_prob_nl = []\n",
    "            for _ in range(f_pass):\n",
    "                outputs = model(inputs)\n",
    "                out_prob.append(F.softmax(outputs, dim=1)) #for selecting positive pseudo-labels\n",
    "                out_prob_nl.append(F.softmax(outputs/args.temp_nl, dim=1)) #for selecting negative pseudo-labels\n",
    "            out_prob = torch.stack(out_prob)\n",
    "            out_prob_nl = torch.stack(out_prob_nl)\n",
    "            out_std = torch.std(out_prob, dim=0)\n",
    "            out_std_nl = torch.std(out_prob_nl, dim=0)\n",
    "            out_prob = torch.mean(out_prob, dim=0)\n",
    "            out_prob_nl = torch.mean(out_prob_nl, dim=0)\n",
    "            max_value, max_idx = torch.max(out_prob, dim=1)\n",
    "            max_std = out_std.gather(1, max_idx.view(-1,1))\n",
    "            out_std_nl = out_std_nl.cpu().numpy()\n",
    "            \n",
    "            #selecting negative pseudo-labels\n",
    "            interm_nl_mask = ((out_std_nl < args.kappa_n) * (out_prob_nl.cpu().numpy() < args.tau_n)) *1\n",
    "\n",
    "            #manually setting the argmax value to zero\n",
    "            for enum, item in enumerate(max_idx.cpu().numpy()):\n",
    "                interm_nl_mask[enum, item] = 0\n",
    "            nl_mask.extend(interm_nl_mask)\n",
    "\n",
    "            idx_list.extend(indexs.numpy().tolist())\n",
    "            gt_list.extend(targets.cpu().numpy().tolist())\n",
    "            target_list.extend(max_idx.cpu().numpy().tolist())\n",
    "\n",
    "            #selecting positive pseudo-labels\n",
    "            if not args.no_uncertainty:\n",
    "                selected_idx = (max_value>=args.tau_p) * (max_std.squeeze(1) < args.kappa_p)\n",
    "            else:\n",
    "                selected_idx = max_value>=args.tau_p\n",
    "\n",
    "            pseudo_maxstd.extend(max_std.squeeze(1)[selected_idx].cpu().numpy().tolist())\n",
    "            pseudo_target.extend(max_idx[selected_idx].cpu().numpy().tolist())\n",
    "            pseudo_idx.extend(indexs[selected_idx].numpy().tolist())\n",
    "            gt_target.extend(targets[selected_idx].cpu().numpy().tolist())\n",
    "\n",
    "            loss = F.cross_entropy(outputs, targets.to(dtype=torch.long))\n",
    "            prec1, prec5 = accuracy(outputs[selected_idx], targets[selected_idx], topk=(1, 5))\n",
    "\n",
    "            losses.update(loss.item(), inputs.shape[0])\n",
    "            top1.update(prec1.item(), inputs.shape[0])\n",
    "            top5.update(prec5.item(), inputs.shape[0])\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if not args.no_progress:\n",
    "                data_loader.set_description(\"Pseudo-Labeling Iter: {batch:4}/{iter:4}. Data: {data:.3f}s. Batch: {bt:.3f}s. Loss: {loss:.4f}. top1: {top1:.2f}. top5: {top5:.2f}. \".format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    iter=len(data_loader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                ))\n",
    "        if not args.no_progress:\n",
    "            data_loader.close()\n",
    "\n",
    "    pseudo_target = np.array(pseudo_target)\n",
    "    gt_target = np.array(gt_target)\n",
    "    pseudo_maxstd = np.array(pseudo_maxstd)\n",
    "    pseudo_idx = np.array(pseudo_idx)\n",
    "\n",
    "    #class balance the selected pseudo-labels\n",
    "    if itr < args.class_blnc-1:\n",
    "        min_count = 5000000 #arbitary large value\n",
    "        for class_idx in range(args.num_classes):\n",
    "            class_len = len(np.where(pseudo_target==class_idx)[0])\n",
    "            if class_len < min_count:\n",
    "                min_count = class_len\n",
    "        min_count = max(25, min_count) #this 25 is used to avoid degenarate cases when the minimum count for a certain class is very low\n",
    "\n",
    "        blnc_idx_list = []\n",
    "        for class_idx in range(args.num_classes):\n",
    "            current_class_idx = np.where(pseudo_target==class_idx)\n",
    "            if len(np.where(pseudo_target==class_idx)[0]) > 0:\n",
    "                current_class_maxstd = pseudo_maxstd[current_class_idx]\n",
    "                sorted_maxstd_idx = np.argsort(current_class_maxstd)\n",
    "                current_class_idx = current_class_idx[0][sorted_maxstd_idx[:min_count]] #select the samples with lowest uncertainty \n",
    "                blnc_idx_list.extend(current_class_idx)\n",
    "\n",
    "        blnc_idx_list = np.array(blnc_idx_list)\n",
    "        pseudo_target = pseudo_target[blnc_idx_list]\n",
    "        pseudo_idx = pseudo_idx[blnc_idx_list]\n",
    "        gt_target = gt_target[blnc_idx_list]\n",
    "\n",
    "    pseudo_labeling_acc = (pseudo_target == gt_target)*1\n",
    "    pseudo_labeling_acc = (sum(pseudo_labeling_acc)/len(pseudo_labeling_acc))*100\n",
    "    print(f'Pseudo-Labeling Accuracy (positive): {pseudo_labeling_acc}, Total Selected: {len(pseudo_idx)}')\n",
    "\n",
    "    pseudo_nl_mask = []\n",
    "    pseudo_nl_idx = []\n",
    "    nl_gt_list = []\n",
    "\n",
    "    for i in range(len(idx_list)):\n",
    "        if idx_list[i] not in pseudo_idx and sum(nl_mask[i]) > 0:\n",
    "            pseudo_nl_mask.append(nl_mask[i])\n",
    "            pseudo_nl_idx.append(idx_list[i])\n",
    "            nl_gt_list.append(gt_list[i])\n",
    "\n",
    "    nl_gt_list = np.array(nl_gt_list)\n",
    "    pseudo_nl_mask = np.array(pseudo_nl_mask)\n",
    "    one_hot_targets = np.eye(args.num_classes)[nl_gt_list]\n",
    "    one_hot_targets = one_hot_targets - 1\n",
    "    one_hot_targets = np.abs(one_hot_targets)\n",
    "    flat_pseudo_nl_mask = pseudo_nl_mask.reshape(1,-1)[0]\n",
    "    flat_one_hot_targets = one_hot_targets.reshape(1,-1)[0]\n",
    "    flat_one_hot_targets = flat_one_hot_targets[np.where(flat_pseudo_nl_mask == 1)]\n",
    "    flat_pseudo_nl_mask = flat_pseudo_nl_mask[np.where(flat_pseudo_nl_mask == 1)]\n",
    "\n",
    "    nl_accuracy = (flat_pseudo_nl_mask == flat_one_hot_targets)*1\n",
    "    nl_accuracy_final = (sum(nl_accuracy)/len(nl_accuracy))*100\n",
    "    print(f'Pseudo-Labeling Accuracy (negative): {nl_accuracy_final}, Total Selected: {len(nl_accuracy)}, Unique Samples: {len(pseudo_nl_mask)}')\n",
    "    pseudo_label_dict = {'pseudo_idx': pseudo_idx.tolist(), 'pseudo_target':pseudo_target.tolist(), 'nl_idx': pseudo_nl_idx, 'nl_mask': pseudo_nl_mask.tolist()}\n",
    " \n",
    "    return losses.avg, top1.avg, pseudo_labeling_acc, len(pseudo_idx), nl_accuracy_final, len(nl_accuracy), len(pseudo_nl_mask), pseudo_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    run_started = datetime.today().strftime('%d-%m-%y_%H%M') #start time to create unique experiment name\n",
    "    parser = argparse.ArgumentParser(description='UPS Training')\n",
    "    parser.add_argument('--out', default=f'outputs', help='directory to output the result')\n",
    "    parser.add_argument('--gpu-id', default='0', type=int,\n",
    "                        help='id(s) for CUDA_VISIBLE_DEVICES')\n",
    "    parser.add_argument('--num-workers', type=int, default=8,\n",
    "                        help='number of workers')\n",
    "    parser.add_argument('--dataset', default='cifar10', type=str,\n",
    "                        choices=['cifar10', 'cifar100'],\n",
    "                        help='dataset names')\n",
    "    parser.add_argument('--n-lbl', type=int, default=4000,\n",
    "                        help='number of labeled data')\n",
    "    parser.add_argument('--arch', default='cnn13', type=str,\n",
    "                        choices=['wideresnet', 'cnn13', 'shakeshake'],\n",
    "                        help='architecture name')\n",
    "    parser.add_argument('--iterations', default=20, type=int,\n",
    "                        help='number of total pseudo-labeling iterations to run')\n",
    "    parser.add_argument('--epchs', default=1024, type=int,\n",
    "                        help='number of total epochs to run')\n",
    "    parser.add_argument('--start-epoch', default=0, type=int,\n",
    "                        help='manual epoch number (useful on restarts)')\n",
    "    parser.add_argument('--batchsize', default=128, type=int,\n",
    "                        help='train batchsize')\n",
    "    parser.add_argument('--lr', '--learning-rate', default=0.03, type=float,\n",
    "                        help='initial learning rate, default 0.03')\n",
    "    parser.add_argument('--warmup', default=0, type=float,\n",
    "                        help='warmup epochs (unlabeled data based)')\n",
    "    parser.add_argument('--wdecay', default=5e-4, type=float,\n",
    "                        help='weight decay')\n",
    "    parser.add_argument('--nesterov', action='store_true', default=True,\n",
    "                        help='use nesterov momentum')\n",
    "    parser.add_argument('--resume', default='', type=str,\n",
    "                        help='path to latest checkpoint (default: none)')\n",
    "    parser.add_argument('--seed', type=int, default=-1,\n",
    "                        help=\"random seed (-1: don't use random seed)\")\n",
    "    parser.add_argument('--no-progress', action='store_true',\n",
    "                        help=\"don't use progress bar\")\n",
    "    parser.add_argument('--dropout', default=0.3, type=float,\n",
    "                        help='dropout probs')\n",
    "    parser.add_argument('--num-classes', default=10, type=int,\n",
    "                        help='total classes')\n",
    "    parser.add_argument('--class-blnc', default=10, type=int,\n",
    "                        help='total number of class balanced iterations')\n",
    "    parser.add_argument('--tau-p', default=0.70, type=float,\n",
    "                        help='confidece threshold for positive pseudo-labels, default 0.70')\n",
    "    parser.add_argument('--tau-n', default=0.05, type=float,\n",
    "                        help='confidece threshold for negative pseudo-labels, default 0.05')\n",
    "    parser.add_argument('--kappa-p', default=0.05, type=float,\n",
    "                        help='uncertainty threshold for positive pseudo-labels, default 0.05')\n",
    "    parser.add_argument('--kappa-n', default=0.005, type=float,\n",
    "                        help='uncertainty threshold for negative pseudo-labels, default 0.005')\n",
    "    parser.add_argument('--temp-nl', default=2.0, type=float,\n",
    "                        help='temperature for generating negative pseduo-labels, default 2.0')\n",
    "    parser.add_argument('--no-uncertainty', action='store_true',\n",
    "                        help='use uncertainty in the pesudo-label selection, default true')\n",
    "    parser.add_argument('--split-txt', default='run1', type=str,\n",
    "                        help='extra text to differentiate different experiments. it also creates a new labeled/unlabeled split')\n",
    "    parser.add_argument('--model-width', default=2, type=int,\n",
    "                        help='model width for WRN-28')\n",
    "    parser.add_argument('--model-depth', default=28, type=int,\n",
    "                        help='model depth for WRN')\n",
    "    parser.add_argument('--test-freq', default=10, type=int,\n",
    "                        help='frequency of evaluations')\n",
    "    \n",
    "\n",
    "    options = (\"--dataset\", \"cifar10\", \"--n-lbl\", \"1000\", \"--class-blnc\", \"7\", \"--split-txt\", \"run1\", \"--arch\", \"cnn13\")\n",
    "    args = parser.parse_args(options)\n",
    "    #print key configurations\n",
    "    print('########################################################################')\n",
    "    print('########################################################################')\n",
    "    print(f'dataset:                                  {args.dataset}')\n",
    "    print(f'number of labeled samples:                {args.n_lbl}')\n",
    "    print(f'architecture:                             {args.arch}')\n",
    "    print(f'number of pseudo-labeling iterations:     {args.iterations}')\n",
    "    print(f'number of epochs:                         {args.epchs}')\n",
    "    print(f'batch size:                               {args.batchsize}')\n",
    "    print(f'lr:                                       {args.lr}')\n",
    "    print(f'value of tau_p:                           {args.tau_p}')\n",
    "    print(f'value of tau_n:                           {args.tau_n}')\n",
    "    print(f'value of kappa_p:                         {args.kappa_p}')\n",
    "    print(f'value of kappa_n:                         {args.kappa_n}')\n",
    "    print('########################################################################')\n",
    "    print('########################################################################')\n",
    "\n",
    "    DATASET_GETTERS = {'cifar10': get_cifar10, 'cifar100': get_cifar100}\n",
    "    exp_name = f'exp_{args.dataset}_{args.n_lbl}_{args.arch}_{args.split_txt}_{args.epchs}_{args.class_blnc}_{args.tau_p}_{args.tau_n}_{args.kappa_p}_{args.kappa_n}_{run_started}'\n",
    "    device = torch.device('cuda', args.gpu_id)\n",
    "    args.device = device\n",
    "    args.exp_name = exp_name\n",
    "    args.dtype = torch.float32\n",
    "    if args.seed != -1:\n",
    "        set_seed(args)\n",
    "    args.out = os.path.join(args.out, args.exp_name)\n",
    "    start_itr = 0\n",
    "\n",
    "    if args.resume and os.path.isdir(args.resume):\n",
    "        resume_files = os.listdir(args.resume)\n",
    "        resume_itrs = [int(item.replace('.pkl','').split(\"_\")[-1]) for item in resume_files if 'pseudo_labeling_iteration' in item]\n",
    "        if len(resume_itrs) > 0:\n",
    "            start_itr = max(resume_itrs)\n",
    "        args.out = args.resume\n",
    "    os.makedirs(args.out, exist_ok=True)\n",
    "    writer = SummaryWriter(args.out)\n",
    "\n",
    "    if args.dataset == 'cifar10':\n",
    "        args.num_classes = 10\n",
    "    elif args.dataset == 'cifar100':\n",
    "        args.num_classes = 100\n",
    "    \n",
    "    for itr in range(start_itr, args.iterations):\n",
    "        if itr == 0 and args.n_lbl < 4000: #use a smaller batchsize to increase the number of iterations\n",
    "            args.batch_size = 64\n",
    "            args.epochs = 1024\n",
    "        else:\n",
    "            args.batch_size = args.batchsize\n",
    "            args.epochs = args.epchs\n",
    "\n",
    "        if os.path.exists(f'data/splits/{args.dataset}_basesplit_{args.n_lbl}_{args.split_txt}.pkl'):\n",
    "            lbl_unlbl_split = f'data/splits/{args.dataset}_basesplit_{args.n_lbl}_{args.split_txt}.pkl'\n",
    "        else:\n",
    "            lbl_unlbl_split = None\n",
    "        \n",
    "        #load the saved pseudo-labels\n",
    "        if itr > 0:\n",
    "            pseudo_lbl_dict = f'{args.out}/pseudo_labeling_iteration_{str(itr)}.pkl'\n",
    "        else:\n",
    "            pseudo_lbl_dict = None\n",
    "        \n",
    "        lbl_dataset, nl_dataset, unlbl_dataset, test_dataset = DATASET_GETTERS[args.dataset]('data/datasets', args.n_lbl,\n",
    "                                                                lbl_unlbl_split, pseudo_lbl_dict, itr, args.split_txt)\n",
    "\n",
    "        model = create_model(args)\n",
    "        model.to(args.device)\n",
    "\n",
    "        nl_batchsize = int((float(args.batch_size) * len(nl_dataset))/(len(lbl_dataset) + len(nl_dataset)))\n",
    "\n",
    "        if itr == 0:\n",
    "            lbl_batchsize = args.batch_size\n",
    "            args.iteration = len(lbl_dataset) // args.batch_size\n",
    "        else:\n",
    "            lbl_batchsize = args.batch_size - nl_batchsize\n",
    "            args.iteration = (len(lbl_dataset) + len(nl_dataset)) // args.batch_size\n",
    "\n",
    "        lbl_loader = DataLoader(\n",
    "            lbl_dataset,\n",
    "            sampler=RandomSampler(lbl_dataset),\n",
    "            batch_size=lbl_batchsize,\n",
    "            num_workers=args.num_workers,\n",
    "            drop_last=True)\n",
    "\n",
    "        nl_loader = DataLoader(\n",
    "            nl_dataset,\n",
    "            sampler=RandomSampler(nl_dataset),\n",
    "            batch_size=nl_batchsize,\n",
    "            num_workers=args.num_workers,\n",
    "            drop_last=True)\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            sampler=SequentialSampler(test_dataset),\n",
    "            batch_size=args.batch_size,\n",
    "            num_workers=args.num_workers)\n",
    "        \n",
    "        unlbl_loader = DataLoader(\n",
    "            unlbl_dataset,\n",
    "            sampler=SequentialSampler(unlbl_dataset),\n",
    "            batch_size=args.batch_size,\n",
    "            num_workers=args.num_workers)\n",
    "\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, nesterov=args.nesterov)\n",
    "        args.total_steps = args.epochs * args.iteration\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, args.warmup * args.iteration, args.total_steps)\n",
    "        start_epoch = 0\n",
    "\n",
    "        if args.resume and itr == start_itr and os.path.isdir(args.resume):\n",
    "            resume_itrs = [int(item.replace('.pth.tar','').split(\"_\")[-1]) for item in resume_files if 'checkpoint_iteration_' in item]\n",
    "            if len(resume_itrs) > 0:\n",
    "                checkpoint_itr = max(resume_itrs)\n",
    "                resume_model = os.path.join(args.resume, f'checkpoint_iteration_{checkpoint_itr}.pth.tar')\n",
    "                if os.path.isfile(resume_model) and checkpoint_itr == itr:\n",
    "                    checkpoint = torch.load(resume_model)\n",
    "                    best_acc = checkpoint['best_acc']\n",
    "                    start_epoch = checkpoint['epoch']\n",
    "                    model.load_state_dict(checkpoint['state_dict'])\n",
    "                    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "                    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "\n",
    "        model.zero_grad()\n",
    "        best_acc = 0\n",
    "        for epoch in range(start_epoch, args.epochs):\n",
    "            if itr == 0:\n",
    "                train_loss = train_initial(args, lbl_loader, model, optimizer, scheduler, epoch, itr)\n",
    "            else:\n",
    "                train_loss = train_regular(args, lbl_loader, nl_loader, model, optimizer, scheduler, epoch, itr)\n",
    "\n",
    "            test_loss = 0.0\n",
    "            test_acc = 0.0\n",
    "            test_model = model\n",
    "            if epoch > (args.epochs+1)/2 and epoch%args.test_freq==0:\n",
    "                test_loss, test_acc = test(args, test_loader, test_model)\n",
    "            elif epoch == (args.epochs-1):\n",
    "                test_loss, test_acc = test(args, test_loader, test_model)\n",
    "\n",
    "            writer.add_scalar('train/1.train_loss', train_loss, (itr*args.epochs)+epoch)\n",
    "            writer.add_scalar('test/1.test_acc', test_acc, (itr*args.epochs)+epoch)\n",
    "            writer.add_scalar('test/2.test_loss', test_loss, (itr*args.epochs)+epoch)\n",
    "\n",
    "            is_best = test_acc > best_acc\n",
    "            best_acc = max(test_acc, best_acc)\n",
    "            model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model_to_save.state_dict(),\n",
    "                'acc': test_acc,\n",
    "                'best_acc': best_acc,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "            }, is_best, args.out, f'iteration_{str(itr)}')\n",
    "    \n",
    "        checkpoint = torch.load(f'{args.out}/checkpoint_iteration_{str(itr)}.pth.tar')\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        model.zero_grad()\n",
    "\n",
    "        #pseudo-label generation and selection\n",
    "        pl_loss, pl_acc, pl_acc_pos, total_sel_pos, pl_acc_neg, total_sel_neg, unique_sel_neg, pseudo_label_dict = pseudo_labeling(args, unlbl_loader, model, itr)\n",
    "\n",
    "        writer.add_scalar('pseudo_labeling/1.regular_loss', pl_loss, itr)\n",
    "        writer.add_scalar('pseudo_labeling/2.regular_acc', pl_acc, itr)\n",
    "        writer.add_scalar('pseudo_labeling/3.pseudo_acc_positive', pl_acc_pos, itr)\n",
    "        writer.add_scalar('pseudo_labeling/4.total_sel_positive', total_sel_pos, itr)\n",
    "        writer.add_scalar('pseudo_labeling/5.pseudo_acc_negative', pl_acc_neg, itr)\n",
    "        writer.add_scalar('pseudo_labeling/6.total_sel_negative', total_sel_neg, itr)\n",
    "        writer.add_scalar('pseudo_labeling/7.unique_samples_negative', unique_sel_neg, itr)\n",
    "\n",
    "        with open(os.path.join(args.out, f'pseudo_labeling_iteration_{str(itr+1)}.pkl'),\"wb\") as f:\n",
    "            pickle.dump(pseudo_label_dict,f)\n",
    "        \n",
    "        with open(os.path.join(args.out, 'log.txt'), 'a+') as ofile:\n",
    "            ofile.write(f'############################# PL Iteration: {itr+1} #############################\\n')\n",
    "            ofile.write(f'Last Test Acc: {test_acc}, Best Test Acc: {best_acc}\\n')\n",
    "            ofile.write(f'PL Acc (Positive): {pl_acc_pos}, Total Selected (Positive): {total_sel_pos}\\n')\n",
    "            ofile.write(f'PL Acc (Negative): {pl_acc_neg}, Total Selected (Negative): {total_sel_neg}, Unique Negative Samples: {unique_sel_neg}\\n\\n')\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "main()"
   ]
  }
 ]
}